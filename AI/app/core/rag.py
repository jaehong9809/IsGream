import warnings
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
print("ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
load_dotenv()

# OpenAI ëª¨ë¸ ì„¤ì •
print("ğŸš€ OpenAI ëª¨ë¸ ì´ˆê¸°í™”...")
llm = ChatOpenAI(model="gpt-4-turbo")

# OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì •
print("ğŸ§  OpenAI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
embedding_function = OpenAIEmbeddings(model="text-embedding-ada-002")
persist_directory = "./chroma_db"

# ChromaDB ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
print("ğŸ“‚ ChromaDB ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘...")
vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)

# Retriever ìƒì„±
print("ğŸ” Retriever ìƒì„± ì¤‘...")
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

prompt = PromptTemplate(
    template="""
    ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ HTP(ì§‘-ë‚˜ë¬´-ì‚¬ëŒ) ê²€ì‚¬ í•´ì„ì„ ìˆ˜í–‰í•˜ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì•„ë˜ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œê¸€ë¡œ ì‹¬ë¦¬ì  í•´ì„ì„ ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

    ### ê²€ì‚¬ ê²°ê³¼
    {context}

    ### ìš”êµ¬ì‚¬í•­
    - ì§‘, ë‚˜ë¬´, ì‚¬ëŒ ê°ê°ì— ëŒ€í•´ ì‹¬ë¦¬ì  ì˜ë¯¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì„¸ìš”.
    - ë¶„ì„ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    - **ê° ë‹µë³€ì˜ ê¸€ì ìˆ˜ë¥¼ ìµœì†Œ 400ì, ìµœëŒ€ 500ìë¡œ ë§ì¶° ì£¼ì„¸ìš”.**
    """,
    input_variables=["context"]
)

# RetrievalQA ì²´ì¸ ìƒì„±
print("ğŸ”— RetrievalQA ì²´ì¸ ìƒì„± ì¤‘...")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",  # ë¬¸ì„œ ê²°í•© ë°©ì‹ (stuff, map_reduce ë“± ê°€ëŠ¥)
    return_source_documents=True
)


# ğŸ” **HTP ê²€ì‚¬ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰**
def process_predictions(query):
    print("ğŸ“ HTP ê²€ì‚¬ ë¶„ì„ ì‹œì‘...")
    start_time = time.time()

    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
    print("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
    try:
        docs = retriever.invoke(query)  # ìµœì‹  ë²„ì „ì—ì„œ ê¶Œì¥ë˜ëŠ” invoke ì‚¬ìš©
        if not docs:
            print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        print(f"âœ… ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    # LLM ì‘ë‹µ ìƒì„±
    start_time = time.time()
    print("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì¤‘...")
    try:
        response = qa_chain.invoke(query)
        print(f"âœ… LLM ì‘ë‹µ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return response["result"]  # `result` í•„ë“œì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
    except Exception as e:
        print(f"âŒ LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"