import warnings
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4-turbo")

# OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_function = OpenAIEmbeddings(model="text-embedding-ada-002")

# ChromaDB ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
vectorstore = Chroma(persist_directory="/home/ubuntu/models/chroma_db", embedding_function=embedding_function)

# Retriever ìƒì„±
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

print("ğŸ“ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜:", vectorstore._collection.count())

prompt = PromptTemplate(
    template="""
    ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ HTP(ì§‘-ë‚˜ë¬´-ì‚¬ëŒ) ê²€ì‚¬ í•´ì„ì„ ìˆ˜í–‰í•˜ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì•„ë˜ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œê¸€ë¡œ ì‹¬ë¦¬ì  í•´ì„ì„ ë§ˆí¬ë‹¤ìš´ ì–¸ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

    ### ê²€ì‚¬ ê²°ê³¼
    {context}

    ### ìš”êµ¬ì‚¬í•­
    - ì§‘, ë‚˜ë¬´, ì‚¬ëŒ ê°ê°ì— ëŒ€í•´ ì‹¬ë¦¬ì  ì˜ë¯¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì„¸ìš”.
    - ë¶„ì„ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    """,
    input_variables=["context"]
)



# RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",  # ë¬¸ì„œ ê²°í•© ë°©ì‹ (stuff, map_reduce ë“± ê°€ëŠ¥)
    return_source_documents=True
)


# ğŸ” **HTP ê²€ì‚¬ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰**
def process_predictions(query):
    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
    docs = retriever.get_relevant_documents(query)
    print(query)
    if not docs:
        print("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„° DBë¥¼ ë‹¤ì‹œ ì €ì¥í•˜ì„¸ìš”.")
        return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

    # ëª¨ë¸ ì‘ë‹µ ë°˜í™˜
    response = qa_chain.invoke(query)

    return response["result"]  # `result` í•„ë“œì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°


